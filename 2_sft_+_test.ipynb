{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rhaveri/master-thesis/blob/main/2_sft_%2B_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prevent widget metadata errors\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "try:\n",
        "    from IPython.display import clear_output\n",
        "    clear_output(wait=True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "print(\" Environment ready\")"
      ],
      "metadata": {
        "id": "J3-OfOlPt5-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --upgrade pyarrow\n",
        "!pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
        "!pip install datasets==2.16.0"
      ],
      "metadata": {
        "id": "99zJ4km37OVl",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLAMA 3 FINE-TUNING FOR NUTRITION RAG\n",
        "\n",
        "!pip show trl\n",
        "\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "# LOAD MODEL WITH LORA\n",
        "\n",
        "def load_model_with_lora(max_seq_length: int = 2048):\n",
        "\n",
        "    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
        "\n",
        "    print(f\"Loading {model_name}...\")\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=model_name,\n",
        "        max_seq_length=max_seq_length,\n",
        "        dtype=None,\n",
        "        load_in_4bit=True\n",
        "    )\n",
        "\n",
        "    # Apply LoRA adapters to attention layers\n",
        "    #  only train q_proj, k_proj, v_proj (query/key/value matrices)\n",
        "    # and MLP layers. This is 1% of total parameters but 80% of learning capacity.\n",
        "    model = FastLanguageModel.get_peft_model(\n",
        "        model,\n",
        "        r=16,  # Rank: Higher = more capacity but slower\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                        \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "        lora_alpha=16,  # Scaling factor (typically equals rank)\n",
        "        lora_dropout=0,  # Dropout (0 for small datasets)\n",
        "        bias=\"none\",\n",
        "        use_gradient_checkpointing=\"unsloth\",  # Saves memory\n",
        "        random_state=3407  # Reproducibility\n",
        "    )\n",
        "\n",
        "    print(\" Model loaded with LoRA adapters\")\n",
        "    return model, tokenizer\n"
      ],
      "metadata": {
        "id": "arMshqPV3gYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# PREPARE TRAINING DATA\n",
        "# Convert JSON messages into LLaMA 3's special token format\n",
        "\n",
        "def prepare_training_dataset(jsonl_file: str, tokenizer, max_seq_length: int = 2048):\n",
        "\n",
        "    dataset = load_dataset(\"json\", data_files=jsonl_file, split=\"train\")\n",
        "    print(f\" Loaded {len(dataset)} training examples\")\n",
        "\n",
        "    def format_conversations(examples):\n",
        "        conversations = examples[\"messages\"]\n",
        "        formatted_texts = [\n",
        "            tokenizer.apply_chat_template(\n",
        "                convo,\n",
        "                tokenize=False,\n",
        "                add_generation_prompt=False\n",
        "            )\n",
        "            for convo in conversations\n",
        "        ]\n",
        "        return {\"text\": formatted_texts}\n",
        "\n",
        "    dataset = dataset.map(format_conversations, batched=True)\n",
        "\n",
        "    # print(\"\\n--- Sample Training Example (first 500 chars) ---\")\n",
        "    print(dataset[0][\"text\"][:500] + \"...\\n\")\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "cOjkRFbDCe86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#  TRAIN THE MODEL\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "def train_model(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    dataset,\n",
        "    max_seq_length: int = 1536,   # safer for T4\n",
        "    max_steps: int = 300,\n",
        "    learning_rate: float = 2e-4,\n",
        "):\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"outputs\",\n",
        "\n",
        "        per_device_train_batch_size=1,   # T4 safe\n",
        "        gradient_accumulation_steps=8,\n",
        "\n",
        "        warmup_steps=10,\n",
        "        max_steps=max_steps,\n",
        "        learning_rate=learning_rate,\n",
        "\n",
        "        fp16=True,\n",
        "\n",
        "        logging_steps=10,\n",
        "        save_steps=100,\n",
        "        save_total_limit=2,\n",
        "\n",
        "        optim=\"adamw_8bit\",\n",
        "        lr_scheduler_type=\"linear\",\n",
        "\n",
        "        report_to=\"none\",\n",
        "        remove_unused_columns=False,\n",
        "    )\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=dataset,\n",
        "\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=max_seq_length,\n",
        "        packing=False,\n",
        "\n",
        "        args=training_args,\n",
        "    )\n",
        "\n",
        "    print(f\"\\n Starting training for {max_steps} steps...\\n\")\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"\\n Training finished!\")\n",
        "\n",
        "    return trainer\n",
        "\n"
      ],
      "metadata": {
        "id": "lenrIIwyBvKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#  SAVE MODEL\n",
        "\n",
        "def save_model_locally(model, tokenizer, output_dir: str = \"lora_model\"):\n",
        "    model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    print(f\" Model saved to '{output_dir}' folder\")\n",
        "\n",
        "\n",
        "def save_to_google_drive(local_folder: str = \"lora_model\",\n",
        "                          drive_folder: str = \"/content/drive/MyDrive/My_Thesis_Model\"):\n",
        "\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "        if not os.path.exists(drive_folder):\n",
        "            os.makedirs(drive_folder)\n",
        "\n",
        "        shutil.copytree(local_folder, f\"{drive_folder}/{local_folder}\", dirs_exist_ok=True)\n",
        "        print(f\" Model saved to: {drive_folder}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Error saving to Drive: {e}\")\n",
        "\n",
        "def download_as_zip(folder: str = \"lora_model\"):\n",
        "    import subprocess\n",
        "    subprocess.run(['zip', '-r', f'{folder}.zip', folder])\n",
        "\n",
        "    from google.colab import files\n",
        "    files.download(f'{folder}.zip')\n",
        "    print(f\" Downloaded {folder}.zip\")\n"
      ],
      "metadata": {
        "id": "fxY7HmqgB2oF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# TEST THE FINE-TUNED MODEL\n",
        "\n",
        "def test_model_with_fake_context(model, tokenizer):\n",
        "\n",
        "    FastLanguageModel.for_inference(model)  # Enable faster generation\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TEST 1: Basic RAG Context Following\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    fake_context = \"\"\"\n",
        "According to the 'Thesis Diet Guidelines 2025', the only healthy fruit is\n",
        "the 'Blue Bananas of Albania'. Eating regular yellow bananas is forbidden.\n",
        "Blue bananas contain magical Vitamin Z.\n",
        "\"\"\"\n",
        "\n",
        "    user_query = \"Are bananas healthy?\"\n",
        "\n",
        "    prompt = f\"\"\"Context information is below.\n",
        "---------------------\n",
        "{fake_context}\n",
        "---------------------\n",
        "Given the context information and not prior knowledge, answer the query.\n",
        "\n",
        "Query: {user_query}\"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a professional AI health coach. Answer strictly based on the provided context.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs,\n",
        "        max_new_tokens=128,\n",
        "        use_cache=True,\n",
        "        temperature=0.1\n",
        "    )\n",
        "\n",
        "    response = tokenizer.batch_decode(outputs)[0]\n",
        "    answer = response.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1].replace(\"<|eot_id|>\", \"\").strip()\n",
        "\n",
        "    print(f\"\\n Question: {user_query}\")\n",
        "    print(f\" Model Answer:\\n{answer}\")\n",
        "\n",
        "    # Test 2: Scientific-sounding fake context\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TEST 2: Professional Context Following\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    scientific_fake_context = \"\"\"\n",
        "ABSTRACT: The 2025 'Alpha-Omega Nutrition Study' (published in J. Thesis Med.)\n",
        "evaluated the effects of 'Lunar-Berries'. The study concluded:\n",
        "1. Consuming 50g of Lunar-Berries reduces fatigue by 40%.\n",
        "2. The berries must be consumed strictly at 8:00 AM.\n",
        "3. Combining the berries with dairy products neutralizes their effect.\n",
        "\"\"\"\n",
        "\n",
        "    user_query = \"What are the findings regarding Lunar-Berries?\"\n",
        "\n",
        "    prompt = f\"\"\"Context information is below.\n",
        "---------------------\n",
        "{scientific_fake_context}\n",
        "---------------------\n",
        "Given the context information and not prior knowledge, answer the query.\n",
        "\n",
        "Query: {user_query}\"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a professional AI health coach. Answer strictly based on the provided context.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs,\n",
        "        max_new_tokens=256,\n",
        "        use_cache=True,\n",
        "        temperature=0.1\n",
        "    )\n",
        "\n",
        "    response = tokenizer.batch_decode(outputs)[0]\n",
        "    answer = response.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1].replace(\"<|eot_id|>\", \"\").strip()\n",
        "\n",
        "    print(f\"\\n Question: {user_query}\")\n",
        "    print(f\" Model Answer:\\n{answer}\")\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\" If the model mentions 'Blue Bananas' or 'Lunar-Berries',\")\n",
        "    print(\"   it successfully learned to follow RAG context!\")\n",
        "    print(\"=\"*50)\n"
      ],
      "metadata": {
        "id": "43g7UDzbBWVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# MAIN EXECUTION PIPELINE\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*60)\n",
        "    print(\"LLAMA 3 FINE-TUNING FOR NUTRITION RAG SYSTEM\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "\n",
        "    model, tokenizer = load_model_with_lora(max_seq_length=2048)\n",
        "\n",
        "    dataset = prepare_training_dataset(\n",
        "        \"training.jsonl\",\n",
        "        tokenizer\n",
        "    )\n",
        "\n",
        "\n",
        "    train_model(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        dataset=dataset,\n",
        "        max_steps=300,  # steps\n",
        "        learning_rate=2e-4\n",
        "    )\n",
        "\n",
        "    save_model_locally(model, tokenizer)\n",
        "\n",
        "    test_model_with_fake_context(model, tokenizer)\n",
        "\n",
        "    # save_to_google_drive()\n",
        "\n",
        "    # download_as_zip()\n",
        "\n",
        "    print(\"\\n COMPLETE! Your fine-tuned model is ready.\")"
      ],
      "metadata": {
        "id": "LstYaw70wqVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_as_zip()"
      ],
      "metadata": {
        "id": "k4BC9sY18tAc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}