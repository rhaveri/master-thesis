{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rhaveri/master-thesis/blob/main/4_ui_chatbot_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XPzK_DAO_AxS"
      },
      "outputs": [],
      "source": [
        "# --- 1. INSTALLATION ---\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
        "!pip install langchain-community langchain-core chromadb langchain-huggingface gradio\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "import gradio as gr\n",
        "\n",
        "\n",
        "if os.path.exists(\"lora_model.zip\"):\n",
        "    !unzip -o -q lora_model.zip -d .\n",
        "    print(\"Unzipped.\")\n",
        "\n",
        "elif os.path.exists(\"lora_model\"):\n",
        "    print(\"Found 'lora_model' folder.\")\n",
        "\n",
        "else:\n",
        "    raise FileNotFoundError(\"Please upload 'lora_model.zip'\")\n",
        "\n",
        "try:\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\",\n",
        "        max_seq_length = 2048,\n",
        "        dtype = None,\n",
        "        load_in_4bit = True,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model)\n",
        "except Exception as e:\n",
        "    print(\" Failed to load model. \")\n",
        "    raise e\n",
        "\n",
        "\n",
        "if not os.path.exists(\"nutrition_documents_v2.json\"):\n",
        "    raise FileNotFoundError(\" Please upload 'nutrition_documents_v2.json' \")\n",
        "\n",
        "with open(\"nutrition_documents_v2.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    docs = json.load(f)\n",
        "\n",
        "documents = [Document(page_content=d[\"text\"], metadata={\"source\": d[\"source\"]}) for d in docs]\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(documents)\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"BAAI/bge-large-en-v1.5\",\n",
        "    model_kwargs={'device': 'cuda'},\n",
        "    encode_kwargs={'batch_size': 32}\n",
        ")\n",
        "vector_db = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
        "\n",
        "print(\"\\n SYSTEM READY\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_logic(message, history):\n",
        "    # 1. Retrieve\n",
        "    docs = vector_db.similarity_search(message, k=3)\n",
        "    if not docs:\n",
        "        return \"I'm sorry, I couldn't find relevant information in my database.\"\n",
        "    context_text = \"\\n\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "    # 2. Prompt\n",
        "    prompt = f\"\"\"Context information is below.\n",
        "---------------------\n",
        "{context_text}\n",
        "---------------------\n",
        "Given the context information and not prior knowledge, answer the query.\n",
        "\n",
        "Query: {message}\"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a professional AI health coach. Answer strictly based on the provided context.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    # 3. Generate\n",
        "    inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(input_ids=inputs, max_new_tokens=512, use_cache=True, temperature=0.3)\n",
        "    response = tokenizer.batch_decode(outputs)[0].split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1].replace(\"<|eot_id|>\", \"\").strip()\n",
        "    return response\n",
        "\n",
        "# UI\n",
        "chatbot = gr.ChatInterface(\n",
        "    fn=chat_logic,\n",
        "    title=\"ðŸ¥— AI Nutrition Health Coach (Thesis Demo)\",\n",
        "    description=\"Ask me about nutrition, sleep, and exercise. I use RAG to find facts from medical journals.\",\n",
        "    examples=[\"How can I eat healthy on a budget?\", \"What is the DASH diet?\", \"How much water do I need?\"],\n",
        "    theme=gr.themes.Soft()\n",
        ")\n",
        "\n",
        "chatbot.launch(share=True)"
      ],
      "metadata": {
        "id": "3YhUyLxc_Dg2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}