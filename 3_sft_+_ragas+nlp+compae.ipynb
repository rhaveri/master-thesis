{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rhaveri/master-thesis/blob/main/3_sft_%2B_ragas%2Bnlp%2Bcompae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prevent widget metadata errors\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "try:\n",
        "    from IPython.display import clear_output\n",
        "    clear_output(wait=True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "print(\"Environment ready\")"
      ],
      "metadata": {
        "id": "foOA9aBet-cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install -q --no-deps xformers trl peft accelerate bitsandbytes\n",
        "!pip install -q langchain-community langchain-core chromadb langchain-huggingface\n",
        "!pip install -q ragas langchain-openai rouge-score bert-score keybert textstat openpyxl evaluate\n",
        "!pip install -q newspaper3k"
      ],
      "metadata": {
        "id": "PZju5ZEHEgKg",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RAGAS EVALUATION SYSTEM\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Dict, Optional\n",
        "from tqdm import tqdm\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import Faithfulness, AnswerRelevancy, ContextPrecision\n",
        "from datasets import Dataset\n",
        "from langchain_openai import ChatOpenAI\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score as bert_score\n",
        "from keybert import KeyBERT\n",
        "import textstat\n",
        "import evaluate as hf_evaluate\n",
        "\n"
      ],
      "metadata": {
        "id": "LrIs8oIRE0zG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD FINE-TUNED MODEL\n",
        "\n",
        "def setup_google_drive():\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    return \"/content/drive/MyDrive\"\n",
        "\n",
        "\n",
        "def load_finetuned_model(max_seq_length: int = 2048):\n",
        "    import os\n",
        "\n",
        "    possible_paths = [\n",
        "        \"lora_model\"\n",
        "    ]\n",
        "\n",
        "    model_location = None\n",
        "\n",
        "    print(\" Searching for model...\")\n",
        "\n",
        "    for path in possible_paths[:2]:\n",
        "        if os.path.exists(path):\n",
        "            model_location = path\n",
        "            print(f\"Found LOCAL model at: {path}\")\n",
        "            break\n",
        "\n",
        "    if not model_location:\n",
        "        print(\" Not found locally. \")\n",
        "        from google.colab import drive\n",
        "        if not os.path.exists(\"/content/drive\"):\n",
        "            drive.mount('/content/drive')\n",
        "\n",
        "        for path in possible_paths[2:]:\n",
        "            if os.path.exists(path):\n",
        "                model_location = path\n",
        "                print(f\" Found DRIVE model at: {path}\")\n",
        "                break\n",
        "\n",
        "    if not model_location:\n",
        "        raise FileNotFoundError(\"Critical Error: Could not find 'lora_model' locally OR in Google Drive. \")\n",
        "\n",
        "    # 3. Load it\n",
        "    print(f\" Loading from: {model_location}\")\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=model_location,\n",
        "        max_seq_length=max_seq_length,\n",
        "        dtype=None,\n",
        "        load_in_4bit=True\n",
        "    )\n",
        "\n",
        "    FastLanguageModel.for_inference(model)\n",
        "    print(\" Model loaded and ready.\")\n",
        "    return model, tokenizer\n",
        "\n"
      ],
      "metadata": {
        "id": "87c2ukq8FDb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#  SETUP RAG VECTOR DATABASE\n",
        "\n",
        "def load_or_copy_file(filename: str, drive_path: str) -> str:\n",
        "\n",
        "    if os.path.exists(filename):\n",
        "        print(f\"Found {filename} locally\")\n",
        "        return filename\n",
        "\n",
        "    drive_file = f\"{drive_path}/{filename}\"\n",
        "    if os.path.exists(drive_file):\n",
        "        os.system(f'cp \"{drive_file}\" .')\n",
        "        return filename\n",
        "\n",
        "    raise FileNotFoundError(f\" Please upload '{filename}' to Colab or Drive\")\n",
        "\n",
        "\n",
        "def build_vector_database(documents_file: str = \"nutrition_documents_v2.json\"):\n",
        "\n",
        "    with open(documents_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        raw_docs = json.load(f)\n",
        "\n",
        "    documents = [\n",
        "        Document(\n",
        "            page_content=doc[\"text\"],\n",
        "            metadata={\"source\": doc[\"source\"]}\n",
        "        )\n",
        "        for doc in raw_docs\n",
        "    ]\n",
        "\n",
        "    # Split into chunks\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    chunks = splitter.split_documents(documents)\n",
        "\n",
        "    print(f\"   Split {len(documents)} documents → {len(chunks)} chunks\")\n",
        "\n",
        "    # Create embeddings\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=\"BAAI/bge-large-en-v1.5\",\n",
        "        model_kwargs={'device': device},\n",
        "        encode_kwargs={'batch_size': 32}\n",
        "    )\n",
        "\n",
        "    # Build ChromaDB\n",
        "    vector_db = Chroma.from_documents(\n",
        "        documents=chunks,\n",
        "        embedding=embeddings,\n",
        "        collection_name=\"finetuned_evaluation\"\n",
        "    )\n",
        "\n",
        "    print(\" Vector database ready\")\n",
        "    return vector_db, embeddings\n"
      ],
      "metadata": {
        "id": "zpn-LRtkFETC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#  GENERATE ANSWERS\n",
        "\n",
        "def generate_answers(model, tokenizer, vector_db, questions: List[str],\n",
        "                     k_docs: int = 3, max_tokens: int = 512) -> List[Dict]:\n",
        "\n",
        "    print(f\"\\n Generating answers for {len(questions)} questions...\")\n",
        "    results = []\n",
        "\n",
        "    for question in tqdm(questions, desc=\"Answering\"):\n",
        "        # Retrieve relevant context\n",
        "        retrieved_docs = vector_db.similarity_search(question, k=k_docs)\n",
        "        context_text = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "        # Format prompt (same as training format)\n",
        "        prompt = f\"\"\"Context information is below.\n",
        "---------------------\n",
        "{context_text}\n",
        "---------------------\n",
        "Given the context information and not prior knowledge, answer the query.\n",
        "\n",
        "Query: {question}\"\"\"\n",
        "\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a professional AI health coach. Answer strictly based on the provided context.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Generate answer\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(\"cuda\")\n",
        "\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            use_cache=True,\n",
        "            temperature=0.1\n",
        "        )\n",
        "\n",
        "        # Extract answer from model output\n",
        "        full_response = tokenizer.batch_decode(outputs)[0]\n",
        "        answer = full_response.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1]\\\n",
        "                              .replace(\"<|eot_id|>\", \"\").strip()\n",
        "\n",
        "        results.append({\n",
        "            \"question\": question,\n",
        "            \"answer\": answer,\n",
        "            \"contexts\": [doc.page_content for doc in retrieved_docs],\n",
        "            \"ground_truth\": answer  # Proxy for Context Precision\n",
        "        })\n",
        "\n",
        "    print(f\" Generated {len(results)} answers\")\n",
        "    return results\n",
        "\n",
        "\n",
        "def save_results(results: List[Dict], filename: str = \"finetuned_eval_data.json\"):\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    print(f\" Results saved to: {filename}\")\n"
      ],
      "metadata": {
        "id": "ynrXBUgUFL1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  RAGAS EVALUATION\n",
        "\n",
        "def evaluate_with_ragas(results_file: str, embeddings,\n",
        "                       use_gpt4: bool = True) -> pd.DataFrame:\n",
        "\n",
        "    # Load results\n",
        "    with open(results_file, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    dataset = Dataset.from_list(data)\n",
        "\n",
        "    # Choose evaluator\n",
        "    if use_gpt4:\n",
        "        print(\"   Using GPT-4o-mini as evaluator\")\n",
        "        if \"OPENAI_API_KEY\" not in os.environ:\n",
        "            import getpass\n",
        "            os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter OpenAI API Key: \")\n",
        "        evaluator_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "    else:\n",
        "        print(\"   Using LLaMA 3 as evaluator\")\n",
        "        from langchain_ollama import ChatOllama\n",
        "        evaluator_llm = ChatOllama(model=\"llama3\", temperature=0)\n",
        "\n",
        "    # Define metrics\n",
        "    metrics = [\n",
        "        Faithfulness(llm=evaluator_llm),\n",
        "        AnswerRelevancy(llm=evaluator_llm, embeddings=embeddings),\n",
        "        ContextPrecision(llm=evaluator_llm)\n",
        "    ]\n",
        "\n",
        "    # Evaluate\n",
        "    results = evaluate(\n",
        "        dataset=dataset,\n",
        "        metrics=metrics,\n",
        "        llm=evaluator_llm,\n",
        "        embeddings=embeddings,\n",
        "        raise_exceptions=False\n",
        "    )\n",
        "\n",
        "    # Save results\n",
        "    df = results.to_pandas()\n",
        "    df.to_csv(\"ragas_results_finetuned.csv\", index=False)\n",
        "    df.to_excel(\"ragas_results_finetuned.xlsx\", index=False)\n",
        "\n",
        "    print(\"\\n RAGAS Evaluation Complete!\")\n",
        "    print(f\"   Avg Faithfulness: {df['faithfulness'].mean():.4f}\")\n",
        "    print(f\"   Avg Relevancy: {df['answer_relevancy'].mean():.4f}\")\n",
        "    print(f\"   Avg Context Precision: {df['context_precision'].mean():.4f}\")\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "5M2FdyYwFcsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  BERTScore\n",
        "\n",
        "def calculate_nlp_metrics(results_file: str) -> pd.DataFrame:\n",
        "\n",
        "\n",
        "    with open(results_file, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Extract answers and contexts\n",
        "    answers = [item['answer'] for item in data]\n",
        "    contexts = [\" \".join(item['contexts']) for item in data]\n",
        "\n",
        "\n",
        "    # Calculate BERTScore\n",
        "    print(\"   Computing BERTScore (takes ~30 seconds)...\")\n",
        "    P, R, F1 = bert_score(answers, contexts, lang=\"en\", verbose=False)\n",
        "\n",
        "    # Create results DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        'bert_score': F1.tolist()\n",
        "    })\n",
        "\n",
        "    df.to_csv(\"nlp_metrics_finetuned.csv\", index=False)\n",
        "\n",
        "    print(f\"\\n NLP Metrics Complete!\")\n",
        "    print(f\"   Avg BERTScore: {F1.mean().item():.4f}\")\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "xyk5Dog8Fqge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  TRAINING DATA QUALITY ANALYSIS\n",
        "\n",
        "def analyze_training_data(excel_file: str) -> pd.DataFrame:\n",
        "\n",
        "\n",
        "    df = pd.read_excel(excel_file)\n",
        "    print(f\"   Loaded {len(df)} training examples\")\n",
        "\n",
        "    # Auto-detect column names\n",
        "    answer_col = 'answer' if 'answer' in df.columns else 'rewritten_answer'\n",
        "\n",
        "    if 'context' not in df.columns:\n",
        "        print(\"  Warning: 'context' column missing. Skipping context-based metrics.\")\n",
        "        has_context = False\n",
        "    else:\n",
        "        has_context = True\n",
        "\n",
        "    # Metric 1: Readability\n",
        "    df['grade_level'] = df[answer_col].apply(\n",
        "        lambda x: textstat.flesch_kincaid_grade(str(x))\n",
        "    )\n",
        "\n",
        "    if has_context:\n",
        "        answers = df[answer_col].astype(str).tolist()\n",
        "        contexts = df['context'].astype(str).tolist()\n",
        "\n",
        "\n",
        "        # Metric 3: BERTScore\n",
        "        P, R, F1 = bert_score(answers, contexts, lang=\"en\", verbose=False)\n",
        "        df['bert_score'] = F1.tolist()\n",
        "\n",
        "        # Metric 4: Keyword Recall\n",
        "        kw_model = KeyBERT()\n",
        "        keyword_recalls = []\n",
        "\n",
        "        for answer, context in tqdm(zip(answers, contexts), total=len(df), desc=\"Keywords\"):\n",
        "            # Extract top keywords from context\n",
        "            keywords = kw_model.extract_keywords(\n",
        "                context,\n",
        "                keyphrase_ngram_range=(1, 2),\n",
        "                stop_words='english',\n",
        "                top_n=10\n",
        "            )\n",
        "\n",
        "            if len(keywords) == 0:\n",
        "                keyword_recalls.append(0)\n",
        "                continue\n",
        "\n",
        "            # Count how many appear in answer\n",
        "            found = sum(1 for kw, _ in keywords if kw in answer.lower())\n",
        "            keyword_recalls.append(found / len(keywords))\n",
        "\n",
        "        df['keyword_recall'] = keyword_recalls\n",
        "\n",
        "    # Save results\n",
        "    output_file = \"training_data_quality_metrics.csv\"\n",
        "    df.to_csv(output_file, index=False)\n",
        "    df.to_excel(\"training_data_quality_metrics.xlsx\", index=False)\n",
        "\n",
        "    print(f\"\\n Training Data Analysis Complete!\")\n",
        "    print(f\"   Avg Grade Level: {df['grade_level'].mean():.2f}\")\n",
        "    if has_context:\n",
        "        print(f\"   Avg BERTScore: {F1.mean().item():.4f}\")\n",
        "        print(f\"   Avg Keyword Recall: {np.mean(keyword_recalls)*100:.1f}%\")\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "vRG_mtvKF1oV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  LOAD METRICS FROM JSON\n",
        "\n",
        "def load_metrics_from_json(json_file: str) -> pd.DataFrame:\n",
        "\n",
        "    with open(json_file, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    if isinstance(data, list):\n",
        "        df = pd.DataFrame(data)\n",
        "    elif isinstance(data, dict):\n",
        "        df = pd.DataFrame([data])\n",
        "    else:\n",
        "        raise ValueError(\" JSON format not recognized\")\n",
        "\n",
        "    print(f\" Loaded {len(df)} entries with {len(df.columns)} metrics\")\n",
        "    print(f\"   Available metrics: {list(df.columns)}\")\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "Q2vfbUlTGA25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# MAIN EXECUTION PIPELINE\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*60)\n",
        "    print(\"COMPREHENSIVE EVALUATION SYSTEM\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Setup\n",
        "    drive_path = setup_google_drive()\n",
        "\n",
        "    # Step 1: Load fine-tuned model\n",
        "    model_path = f\"{drive_path}/My_Thesis_Model\"\n",
        "    model, tokenizer = load_finetuned_model()\n",
        "\n",
        "    # Step 2: Setup RAG database\n",
        "    docs_file = load_or_copy_file(\"nutrition_documents_v2.json\", drive_path)\n",
        "    vector_db, embeddings = build_vector_database(docs_file)\n",
        "\n",
        "    # Step 3: Load questions\n",
        "    questions_file = load_or_copy_file(\"questions.json\", drive_path)\n",
        "    with open(questions_file, \"r\") as f:\n",
        "        questions = json.load(f)\n",
        "\n",
        "    # Step 4: Generate answers\n",
        "    results = generate_answers(model, tokenizer, vector_db, questions)\n",
        "    save_results(results, \"finetuned_eval_data.json\")\n",
        "\n",
        "    # Step 5: Evaluate with RAGAS\n",
        "    ragas_df = evaluate_with_ragas(\"finetuned_eval_data.json\", embeddings, use_gpt4=True)\n",
        "\n",
        "    # Step 6: Calculate NLP metrics\n",
        "    nlp_df = calculate_nlp_metrics(\"finetuned_eval_data.json\")\n",
        "\n",
        "    # Step 7: Analyze training data (if available)\n",
        "    # training_df = analyze_training_data(\"dataset_with_context_backup.xlsx\")\n",
        "\n",
        "    #  Load pre-calculated metrics\n",
        "    # metrics_df = load_metrics_from_json(\"all_metrics.json\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"EVALUATION COMPLETE!\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"\\nGenerated files:\")\n",
        "    print(\"  • finetuned_eval_data.json - Generated answers\")\n",
        "    print(\"  • ragas_results_finetuned.csv/xlsx - RAGAS metrics\")\n",
        "    print(\"  • nlp_metrics_finetuned.csv - ROUGE & BERTScore\")\n",
        "    print(\"  • training_data_quality_metrics.csv/xlsx - Training analysis\")"
      ],
      "metadata": {
        "id": "fd82q7eRGHhu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}